# 3.6 Evaluation

Take what we have designed and get user feedback.

Methods for evaluation:

- Early: Qualitative
- Later: Does it help their workflow?
- Last: Empirical, quantitative evaluation
- Predictive evaluation

## Three types of Evaluation strategies

1. Qualitative Evaluation — What do users like, dislike, what's easy?
2. Empirical Evaluation-Controlled experiments evaluated quantitatively with many users.
3. Predictive Evaluation — Evaluation without users. Faster than having to deal with users.

## Evaluation Terminology

- Reliability—whether a measure is consistent over time. We want high reliability
- Validity—How accurately an evaluation measures reality.
- Generalizable-The extent to which we can apply our lessons learned to broader people.
- Precision-How specific the assessment is?

## Five Tips: What to Evaluate

1. Efficiency—how long does it take for users to complete certain tasks?
2. Accuracy-how many errors do users make?
3. Learnability-how long does it take a novice user to attain expertise?
4. Memorability — how well does a user remember about the interface after a break?
5. Satisfaction - avoid social desirability.

Articulate what data you're gathering, why, and what methods you will use.

## Evaluation Timeline

Over time, the methods we use when evaluating will change.

In the beginning, they are formative. Intention is to improve the interface going forward.
More qualitative. Lab testing.

In the end, they are summative. Say concretely at the end what our result is.
More empirical, assess changes. Test in situ—in the field with more finished product.

Predictive also help generate ideas and inform changes that should happen over-time.

## Evaluation Design

To ensure that our evaluation is successful:

1. Define the task-context, constraints, tasks.
2. Define performance measures—how are we going to evaluate the user's performance? Helps avoid confirmation bias.
   Forces us to objectively analyze.
3. Develop the experiment—how are we going to gather the data? What will users do? What will be measured?
4. Recruit Participants—make sure they're aware of their rights
5. Do the experiment
6. Analyze the data—focus on what the data tells us
7. Summarize the data to inform our ongoing design process

We will make this part of our design life cycle.

## Qualitative Evaluation

What did you like/dislike/think? What was your goal?

Use methods like we used for need-finding. Interviews, thank aloud protocols, focus groups, surveys.

### Designing a Qualitative Evaluation

To design - answer these questions:

1. Prior experience or live demonstration?
2. Synchronous or asynchronous? Synchronous might be more beneficial, but asynchronous is easier to carry out.
3. One interface vs. multiple prototypes? If multiple, make sure to vary the feedback.
4. Think aloud protocol vs. post-event protocol? post-event makes you wait until the end to give feedback, but
   think-aloud might introduce new biases.
5. Individuals vs. groups? Groups can build on each other, but introduce biases. Individuals are not biased.

### Capturing a Qualitative Evaluation

1. Record session
    - Pros: Automated, Comprehensive, Passive
    - Cons: Intrusive, Non-Analyzable, Screen-Less
2. Note-taking
    - Pros: Cheap, Non-Intrusive, Analyzable
    - Cons: Slow, Manual, Limited in information captured
3. Software Logging
    - Pros: Automated, Passive, Analyzable
    - Cons: Limited, Narrow, Tech-Sensitive (need a working prototype already to test it)

### Five Tips for Qualitative Evaluations

1. Run pilot studies—try experiments with family/friends before real-users
2. Focus on feedback
3. Use questions
4. Tell users what to do, but not how to do it
5. Capture satisfaction—ask if they like it

## Empirical Evaluation

Capture something numeric and interpret. Come to something that is verifiable and conclusive.

Used within the industry to compare designs. In research, used as the basis for new theories to be built.

Involves comparisons—objectively. How can we show that there is a difference between two different designs?

### Designing Empirical Evaluation

Treatments—multiple conditions—what a participant does during the experiment. We investigate comparisons between
treatments and make conclusions about how the treatments are different.

Decide if each participant participates in one or both treatments.

**Between Subject Design—**comparison of data from one group to participants in another group.

**Within Subjects Design—**Comparison within one group experiencing multiple treatments. The first treatment will bias,
so we want to make sure that we will randomly assign them to the order of treatments. Allows us to gather even more
data. Allows us to do within subjects and between subjects. Requires more time from participants.

**Random Assignment—**using random choice to determine who gets which treatment. is required.

### Hypothesis Testing

Test whether the data allows us to confirm that the opposite is true.

Statistically significant(p-value < 0.05)

## Quantitative Data
